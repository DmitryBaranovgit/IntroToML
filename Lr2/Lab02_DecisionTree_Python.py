# -*- coding: utf-8 -*-
"""Баранов Д.А. ИВТ 2.1. ЛР №2. DecisionTree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sLAMnaCt9c9FStWfD23hGfYFebntnEs9

Баранов Д.А. ИВТ 2.1
Лабораторная работа №2. DecisionTree
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree, model_selection, metrics
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import GridSearchCV, StratifiedKFold

voice_data = pd.read_csv('/content/voice_gender.csv')
print(voice_data.head())
print("Пропущенные значения:", voice_data.isnull().sum().sum())

# Подготовка признаков
X = voice_data.drop('label', axis = 1)
y = voice_data['label']

# Разделение
X_train, X_test, y_train, y_test = model_selection.train_test_split(
    X, y, test_size = 0.2, stratify = y, random_state = 42
)
print(f'Train shape: {X_train.shape}, Test shape: {X_test.shape}')

"""Задание 1. Решающие пни (max_depth = 1)"""

dt1 = DecisionTreeClassifier(criterion = 'entropy', max_depth = 1, random_state = 42)
dt1.fit(X_train, y_train)
plt.figure(figsize = (10, 5))
plot_tree(dt1, feature_names = X.columns, class_names = dt1.classes_, filled = True)
plt.title("Decision Tree - Depth 1")
plt.show()

# Вопросы к заданию 1
print("Признак в корне:", X.columns[dt1.tree_.feature[0]])
print("Пороговое значение:", round(dt1.tree_.threshold[0], 3))
samples_left = dt1.tree_.n_node_samples[1]
total = X_train.shape[0]
print("Процент в обучающей выборке (левое правило): ", round(samples_left / total * 100, 1))
y_pred1 = dt1.predict(X_test)
acc1 = metrics.accuracy_score(y_test, y_pred1)
print("Accuracy (depth = 1):", round(acc1, 3))

"""Задание 2: Дерево глубины 2"""

dt2 = DecisionTreeClassifier(criterion = 'entropy', max_depth = 2, random_state = 42)
dt2.fit(X_train, y_train)
plt.figure(figsize = (12, 6))
plot_tree(dt2, feature_names = X.columns, class_names = dt2.classes_, filled = True)
plt.title("Decision Tree - Depth 2")
plt.show()

print("Используемые признаки (глубина 2):", set(X.columns[dt2.tree_.feature[dt2.tree_.feature >= 0]]))
print("Число листьев, предсказывающих female:", list(dt2.apply(X_train)).count(2))
y_pred2 = dt2.predict(X_test)
acc2 = metrics.accuracy_score(y_test, y_pred2)
print("Accuracy (depth = 2):", round(acc2, 3))

"""Задание 3: Без ограничения глубины"""

dt3 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
dt3.fit(X_train, y_train)
print("Глубина деерева:", dt3.get_depth())
print("Количество листьев:", dt3.get_n_leaves())
acc_train3 = metrics.accuracy_score(y_train, dt3.predict(X_train))
acc_test3 = metrics.accuracy_score(y_test, dt3.predict(X_test))
print("Accuracy (train):", round(acc_train3, 3))
print("Accuracy (test):", round(acc_test3, 3))

"""Задание 4: GridSearchCV"""

param_grid = {
    'criterion': ['gini', 'entropy'], # критерий информативности
    'max_depth': list(range(4, 11)), # максимальная глубина дерева
    'min_samples_split': [3, 4, 5, 10] # минимальное количетсво объектов, необходимое для сплита
}
cv = StratifiedKFold(n_splits = 5)
grid = GridSearchCV(
    DecisionTreeClassifier(random_state = 0),
    param_grid,
    scoring = 'accuracy',
    cv = cv
)
grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Лучший критерий:", grid.best_params_['criterion'])
print("Оптимальная глубина:", grid.best_params_['max_depth'])
print("Оптимальный min_samples_split:", grid.best_params_['min_samples_split'])

acc_train_best = metrics.accuracy_score(y_train, best_model.predict(X_train))
acc_test_best = metrics.accuracy_score(y_test, best_model.predict(X_test))
print("Accuracy (train, best model):", round(acc_train_best, 3))
print("Accuracy (test, best model):", round(acc_test_best, 3))

"""Задание 5: Важность признаков"""

importances = best_model.feature_importances_
importance_df = pd.Series(importances, index = X.columns).sort_values(ascending = False)
print("Топ-3 признака:\n", importance_df.head(3))

# Визуализация
plt.figure(figsize = (12, 6))
sns.barplot(x = importance_df.values, y = importance_df.index)
plt.title('Feature Importances (Best Tree)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()